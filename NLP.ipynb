{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i': 1, 'love': 2}\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "tokenizer=Tokenizer()\n",
    "data=\" I Love\"\n",
    "corpus= data.lower().split(\"\\n\")\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "total_words=len(tokenizer.word_index)+1\n",
    "print(tokenizer.word_index)\n",
    "print(total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=['I love This world',\n",
    "           'World is so beautiful!'\n",
    "          ]\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index =tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i': 1, 'love': 2, 'world': 3, 'this': 4, 'is': 5, 'so': 6, 'beautiful': 7}\n"
     ]
    }
   ],
   "source": [
    "print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences= tokenizer.texts_to_sequences(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 4, 3], [3, 5, 6, 7]]\n"
     ]
    }
   ],
   "source": [
    "print(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences1=['I  really love my dog',\n",
    "           'my dog loves my manatee'\n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(sentences1)\n",
    "word_index =tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'my': 1, 'dog': 2, 'i': 3, 'love': 4, 'really': 5, 'loves': 6, 'manatee': 7, 'world': 8, 'this': 9, 'is': 10, 'so': 11, 'beautiful': 12}\n"
     ]
    }
   ],
   "source": [
    "print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences1= tokenizer.texts_to_sequences(sentences1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 5, 4, 1, 2], [1, 2, 6, 1, 7]]\n"
     ]
    }
   ],
   "source": [
    "print (sequences1)"
   ]
  },
 
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'my': 1, 'love': 2, 'dog': 3, 'i': 4, 'you': 5, 'cat': 6, 'do': 7, 'think': 8, 'is': 9, 'amazing': 10}\n",
      "[[4, 2, 1, 3], [4, 2, 1, 6], [5, 2, 1, 3], [7, 5, 8, 1, 3, 9, 10]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "train_data=['I  love my dog',\n",
    "           'I love my cat',\n",
    "            'You love my dog!',\n",
    "            'Do you think my dog is amazing?'\n",
    "            \n",
    "          ]\n",
    "tokenizer=Tokenizer(num_words=100)\n",
    "tokenizer.fit_on_texts(train_data)\n",
    "word_index =tokenizer.word_index\n",
    "T1=tokenizer.texts_to_sequences(train_data)\n",
    "print(word_index)\n",
    "print(T1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'my': 1, 'love': 2, 'dog': 3, 'i': 4, 'you': 5, 'cat': 6, 'do': 7, 'think': 8, 'is': 9, 'amazing': 10}\n",
      "[[4, 2, 1, 3], [1, 3, 1]]\n"
     ]
    }
   ],
   "source": [
    "test_data=['I  really love my dog',\n",
    "           'my dog loves my manatee'\n",
    "          ]\n",
    "T2=tokenizer.texts_to_sequences(test_data)\n",
    "print(word_index)\n",
    "print(T2)"
   ]
  },
  
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<00v>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n",
      "[[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "train_data=['I  love my dog',\n",
    "           'I love my cat',\n",
    "            'You love my dog!',\n",
    "            'Do you think my dog is amazing?'\n",
    "            \n",
    "          ]\n",
    "tokenizer=Tokenizer(num_words=100,oov_token=\"<00v>\")\n",
    "tokenizer.fit_on_texts(train_data)\n",
    "word_index =tokenizer.word_index\n",
    "T1=tokenizer.texts_to_sequences(train_data)\n",
    "print(word_index)\n",
    "print(T1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<00v>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n",
      "[[5, 1, 3, 2, 4], [2, 4, 1, 2, 1]]\n"
     ]
    }
   ],
   "source": [
    "test_data=['I  really love my dog',\n",
    "           'my dog loves my manatee'\n",
    "          ]\n",
    "T2=tokenizer.texts_to_sequences(test_data)\n",
    "print(word_index)\n",
    "print(T2)"
   ]
  },
  
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<00v>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n",
      "[[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]\n",
      "[[ 0  0  0  5  3  2  4]\n",
      " [ 0  0  0  5  3  2  7]\n",
      " [ 0  0  0  6  3  2  4]\n",
      " [ 8  6  9  2  4 10 11]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "train_data=['I  love my dog',\n",
    "           'I love my cat',\n",
    "            'You love my dog!',\n",
    "            'Do you think my dog is amazing?'\n",
    "            \n",
    "          ]\n",
    "tokenizer=Tokenizer(num_words=100,oov_token=\"<00v>\")\n",
    "tokenizer.fit_on_texts(train_data)\n",
    "word_index =tokenizer.word_index\n",
    "Seq=tokenizer.texts_to_sequences(train_data)\n",
    "padded=pad_sequences(Seq)\n",
    "print(word_index)\n",
    "print(Seq)\n",
    "print(padded)"
   ]
  },
  
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<00v>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n",
      "[[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]\n",
      "[[ 5  3  2  4  0  0  0]\n",
      " [ 5  3  2  7  0  0  0]\n",
      " [ 6  3  2  4  0  0  0]\n",
      " [ 8  6  9  2  4 10 11]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "train_data=['I  love my dog',\n",
    "           'I love my cat',\n",
    "            'You love my dog!',\n",
    "            'Do you think my dog is amazing?'\n",
    "            \n",
    "          ]\n",
    "tokenizer=Tokenizer(num_words=100,oov_token=\"<00v>\")\n",
    "tokenizer.fit_on_texts(train_data)\n",
    "word_index =tokenizer.word_index\n",
    "Seq=tokenizer.texts_to_sequences(train_data)\n",
    "padded=pad_sequences(Seq, padding='post')\n",
    "print(word_index)\n",
    "print(Seq)\n",
    "print(padded)"
   ]
  },
  
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<00v>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n",
      "[[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]\n",
      "[[ 0  5  3  2  4]\n",
      " [ 0  5  3  2  7]\n",
      " [ 0  6  3  2  4]\n",
      " [ 9  2  4 10 11]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "train_data=['I  love my dog',\n",
    "           'I love my cat',\n",
    "            'You love my dog!',\n",
    "            'Do you think my dog is amazing?'\n",
    "            \n",
    "          ]\n",
    "tokenizer=Tokenizer(num_words=100,oov_token=\"<00v>\")\n",
    "tokenizer.fit_on_texts(train_data)\n",
    "word_index =tokenizer.word_index\n",
    "Seq=tokenizer.texts_to_sequences(train_data)\n",
    "padded=pad_sequences(Seq,  maxlen=5)\n",
    "print(word_index)\n",
    "print(Seq)\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-----------Training and Testing-------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "{\"article_link\": \"https://www.huffingtonpost.com/entry/versace-black-code_us_5861fbefe4b0de3a08f600d5\", \"headline\": \"former versace store clerk sues over secret 'black code' for minority shoppers\", \"is_sarcastic\": 0},\n",
      "{\"article_link\": \"https://www.huffingtonpost.com/entry/roseanne-revival-review_us_5ab3a497e4b054d118e04365\", \"headline\": \"the 'roseanne' revival catches up to our thorny political mood, for better and worse\", \"is_sarcastic\": 0},\n",
      "[308, 15115, 679, 3337, 2298, 48, 382, 2576, 15116, 6, 2577, 8434]\n",
      "[  308 15115   679  3337  2298    48   382  2576 15116     6  2577  8434\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0]\n",
      "(26709, 40)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "\n",
    "datastore = requests.get('https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sarcasm.json')\n",
    "print(datastore.text[0:450])\n",
    "sentences = []\n",
    "labels = []\n",
    "urls = []\n",
    "\n",
    "for item in datastore.json():\n",
    "    sentences.append(item['headline'])\n",
    "    labels.append(item['is_sarcastic'])\n",
    "    urls.append(item['article_link'])\n",
    "\n",
    "tokenizer=Tokenizer(oov_token=\"<00v>\")\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index =tokenizer.word_index\n",
    "#print(word_index)\n",
    "Seq=tokenizer.texts_to_sequences(sentences)\n",
    "padded=pad_sequences(Seq , padding='post')\n",
    "print(Seq[0])\n",
    "print(padded[0])\n",
    "print(padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20032\n",
      "26709\n"
     ]
    }
   ],
   "source": [
    "training_size = round(len(sentences) * .75)\n",
    "print(training_size)\n",
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sentences = sentences[0:training_size]\n",
    "testing_sentences = sentences[training_size:]\n",
    "training_labels = labels[0:training_size]\n",
    "testing_labels = labels[training_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(10000,oov_token='oov_tok')\n",
    "tokenizer.fit_on_texts(training_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "training_padded = pad_sequences(training_sequences, maxlen= 100, padding='post', truncating='post')\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
    "testing_padded = pad_sequences(testing_sequences, maxlen= 100, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 16)           160000    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 24)                408       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 25        \n",
      "=================================================================\n",
      "Total params: 160,433\n",
      "Trainable params: 160,433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 16\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(10000, embedding_dim, input_length=100),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(24, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20032 samples, validate on 6677 samples\n",
      "Epoch 1/30\n",
      "20032/20032 - 4s - loss: 0.6578 - acc: 0.6027 - val_loss: 0.5407 - val_acc: 0.8016\n",
      "Epoch 2/30\n",
      "20032/20032 - 4s - loss: 0.3935 - acc: 0.8435 - val_loss: 0.3706 - val_acc: 0.8426\n",
      "Epoch 3/30\n",
      "20032/20032 - 4s - loss: 0.2876 - acc: 0.8848 - val_loss: 0.3450 - val_acc: 0.8564\n",
      "Epoch 4/30\n",
      "20032/20032 - 4s - loss: 0.2389 - acc: 0.9052 - val_loss: 0.3433 - val_acc: 0.8558\n",
      "Epoch 5/30\n",
      "20032/20032 - 3s - loss: 0.2049 - acc: 0.9213 - val_loss: 0.3655 - val_acc: 0.8436\n",
      "Epoch 6/30\n",
      "20032/20032 - 4s - loss: 0.1797 - acc: 0.9308 - val_loss: 0.3632 - val_acc: 0.8544\n",
      "Epoch 7/30\n",
      "20032/20032 - 3s - loss: 0.1578 - acc: 0.9414 - val_loss: 0.3942 - val_acc: 0.8453\n",
      "Epoch 8/30\n",
      "20032/20032 - 4s - loss: 0.1407 - acc: 0.9496 - val_loss: 0.4045 - val_acc: 0.8510\n",
      "Epoch 9/30\n",
      "20032/20032 - 3s - loss: 0.1263 - acc: 0.9539 - val_loss: 0.4221 - val_acc: 0.8504\n",
      "Epoch 10/30\n",
      "20032/20032 - 3s - loss: 0.1131 - acc: 0.9608 - val_loss: 0.4588 - val_acc: 0.8438\n",
      "Epoch 11/30\n",
      "20032/20032 - 4s - loss: 0.1023 - acc: 0.9650 - val_loss: 0.4756 - val_acc: 0.8466\n",
      "Epoch 12/30\n",
      "20032/20032 - 4s - loss: 0.0934 - acc: 0.9683 - val_loss: 0.5409 - val_acc: 0.8302\n",
      "Epoch 13/30\n",
      "20032/20032 - 3s - loss: 0.0852 - acc: 0.9717 - val_loss: 0.5339 - val_acc: 0.8405\n",
      "Epoch 14/30\n",
      "20032/20032 - 3s - loss: 0.0756 - acc: 0.9751 - val_loss: 0.5989 - val_acc: 0.8314\n",
      "Epoch 15/30\n",
      "20032/20032 - 3s - loss: 0.0699 - acc: 0.9761 - val_loss: 0.6056 - val_acc: 0.8354\n",
      "Epoch 16/30\n",
      "20032/20032 - 4s - loss: 0.0610 - acc: 0.9804 - val_loss: 0.6456 - val_acc: 0.8311\n",
      "Epoch 17/30\n",
      "20032/20032 - 3s - loss: 0.0588 - acc: 0.9811 - val_loss: 0.6820 - val_acc: 0.8275\n",
      "Epoch 18/30\n",
      "20032/20032 - 4s - loss: 0.0526 - acc: 0.9832 - val_loss: 0.7251 - val_acc: 0.8267\n",
      "Epoch 19/30\n",
      "20032/20032 - 3s - loss: 0.0477 - acc: 0.9854 - val_loss: 0.7690 - val_acc: 0.8231\n",
      "Epoch 20/30\n",
      "20032/20032 - 4s - loss: 0.0445 - acc: 0.9861 - val_loss: 0.7927 - val_acc: 0.8230\n",
      "Epoch 21/30\n",
      "20032/20032 - 4s - loss: 0.0411 - acc: 0.9874 - val_loss: 0.9582 - val_acc: 0.8096\n",
      "Epoch 22/30\n",
      "20032/20032 - 3s - loss: 0.0386 - acc: 0.9876 - val_loss: 0.8674 - val_acc: 0.8207\n",
      "Epoch 23/30\n",
      "20032/20032 - 3s - loss: 0.0325 - acc: 0.9909 - val_loss: 0.9160 - val_acc: 0.8176\n",
      "Epoch 24/30\n",
      "20032/20032 - 3s - loss: 0.0330 - acc: 0.9898 - val_loss: 0.9667 - val_acc: 0.8137\n",
      "Epoch 25/30\n",
      "20032/20032 - 6s - loss: 0.0273 - acc: 0.9923 - val_loss: 0.9905 - val_acc: 0.8153\n",
      "Epoch 26/30\n",
      "20032/20032 - 4s - loss: 0.0269 - acc: 0.9914 - val_loss: 1.0592 - val_acc: 0.8122\n",
      "Epoch 27/30\n",
      "20032/20032 - 3s - loss: 0.0244 - acc: 0.9936 - val_loss: 1.0686 - val_acc: 0.8132\n",
      "Epoch 28/30\n",
      "20032/20032 - 4s - loss: 0.0217 - acc: 0.9942 - val_loss: 1.1488 - val_acc: 0.8138\n",
      "Epoch 29/30\n",
      "20032/20032 - 4s - loss: 0.0222 - acc: 0.9929 - val_loss: 1.1524 - val_acc: 0.8101\n",
      "Epoch 30/30\n",
      "20032/20032 - 3s - loss: 0.0186 - acc: 0.9951 - val_loss: 1.2270 - val_acc: 0.8123\n"
     ]
    }
   ],
   "source": [
    "training_padded = np.array(training_padded)\n",
    "training_labels = np.array(training_labels)\n",
    "testing_padded = np.array(testing_padded)\n",
    "testing_labels = np.array(testing_labels)\n",
    "# Training the model\n",
    "num_epochs = 30\n",
    "history = model.fit(training_padded, training_labels, epochs=num_epochs, validation_data=(testing_padded, testing_labels), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9997178 ]\n",
      " [0.16032079]]\n"
     ]
    }
   ],
   "source": [
    "sentence=[ \"Please try to schedule meetings around my job interviews.\", \n",
    "          \"I send pointless emails late at night to impress coworkers.\"]\n",
    "sequences = tokenizer.texts_to_sequences(sentence)\n",
    "padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "print(model.predict(padded))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
